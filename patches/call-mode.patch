diff --git a/src/lib/components/chat/MessageInput/CallOverlay.svelte b/src/lib/components/chat/MessageInput/CallOverlay.svelte
index 41cc06624..62ceda97e 100644
--- a/src/lib/components/chat/MessageInput/CallOverlay.svelte
+++ b/src/lib/components/chat/MessageInput/CallOverlay.svelte
@@ -46,6 +46,19 @@
 	let videoInputDevices = [];
 	let selectedVideoInputDeviceId = null;
 
+	const CHUNK_MS = 12000;
+	const MAX_CONCURRENT_UPLOADS = 2;
+	const MAX_BACKLOG = 6;
+	const MIN_BLOB_BYTES = 8192;
+	let chunkIntervalId = null;
+	let transcriptBuffer = [];
+	let uploadQueue = [];
+	let inFlight = 0;
+	let stopping = false;
+	let warnedDrop = false;
+	const chunkedMode = ($settings?.audio?.call?.chunked ?? true);
+	const isIPhone = typeof navigator !== 'undefined' && /iPhone/i.test(navigator.userAgent || '');
+
 	const getVideoInputDevices = async () => {
 		const devices = await navigator.mediaDevices.enumerateDevices();
 		videoInputDevices = devices.filter((device) => device.kind === 'videoinput');
@@ -224,31 +237,74 @@
 			if (!audioStream) {
 				audioStream = await navigator.mediaDevices.getUserMedia({
 					audio: {
-						echoCancellation: true,
-						noiseSuppression: true,
-						autoGainControl: true
+						channelCount: 1,
+						echoCancellation: false,
+						noiseSuppression: false,
+						autoGainControl: false
 					}
 				});
 			}
-			mediaRecorder = new MediaRecorder(audioStream);
+
+			const mimeTypes = isIPhone
+				? ['audio/mp4', 'audio/webm;codecs=opus', 'audio/webm']
+				: ['audio/webm;codecs=opus', 'audio/webm', 'audio/mp4'];
+			const mimeType = mimeTypes.find((t) => MediaRecorder.isTypeSupported(t));
+			mediaRecorder = mimeType ? new MediaRecorder(audioStream, { mimeType }) : new MediaRecorder(audioStream);
 
 			mediaRecorder.onstart = () => {
 				console.log('Recording started');
-				audioChunks = [];
+				if (chunkIntervalId) clearInterval(chunkIntervalId);
+				if (!isIPhone) {
+					chunkIntervalId = setInterval(() => {
+						if (!stopping) {
+							if (uploadQueue.length + inFlight > MAX_BACKLOG) {
+								if (!warnedDrop) {
+									console.log('Backlog high, skipping requestData tick');
+									warnedDrop = true;
+								}
+								return;
+							}
+							try {
+								mediaRecorder.requestData();
+							} catch (e) {
+								console.error(e);
+							}
+						}
+					}, CHUNK_MS);
+				}
 			};
 
 			mediaRecorder.ondataavailable = (event) => {
-				if (hasStartedSpeaking) {
-					audioChunks.push(event.data);
+				const blob = event.data;
+				if (!blob || blob.size < MIN_BLOB_BYTES) {
+					return;
+				}
+				if (isIPhone) {
+					audioChunks.push(blob);
+					return;
 				}
+				if (uploadQueue.length + inFlight > MAX_BACKLOG) {
+					if (!warnedDrop) {
+						console.log('Dropping chunk due to backlog');
+						warnedDrop = true;
+					}
+					return;
+				}
+				uploadQueue.push(blob);
+				processQueue();
 			};
 
 			mediaRecorder.onstop = (e) => {
 				console.log('Recording stopped', audioStream, e);
-				stopRecordingCallback();
 			};
 
 			analyseAudio(audioStream);
+			try {
+				mediaRecorder.start();
+			} catch (error) {
+				console.error('Error starting recording:', error);
+				toast.error($i18n.t('Error starting recording.'));
+			}
 		}
 	};
 
@@ -296,7 +352,7 @@
 		let lastSoundTime = Date.now();
 		hasStartedSpeaking = false;
 
-		console.log('ðŸ”Š Sound detection started', lastSoundTime, hasStartedSpeaking);
+		console.log(' Sound detection started', lastSoundTime, hasStartedSpeaking);
 
 		const detectSound = () => {
 			const processFrame = () => {
@@ -305,7 +361,6 @@
 				}
 
 				if (assistantSpeaking && !($settings?.voiceInterruption ?? false)) {
-					// Mute the audio if the assistant is speaking
 					analyser.maxDecibels = 0;
 					analyser.minDecibels = -1;
 				} else {
@@ -316,39 +371,18 @@
 				analyser.getByteTimeDomainData(timeDomainData);
 				analyser.getByteFrequencyData(domainData);
 
-				// Calculate RMS level from time domain data
+				// Calculate RMS level from time domain data for UI
 				rmsLevel = calculateRMS(timeDomainData);
 
-				// Check if initial speech/noise has started
 				const hasSound = domainData.some((value) => value > 0);
 				if (hasSound) {
-					// BIG RED TEXT
-					console.log('%c%s', 'color: red; font-size: 20px;', 'ðŸ”Š Sound detected');
-					if (mediaRecorder && mediaRecorder.state !== 'recording') {
-						mediaRecorder.start();
-					}
-
 					if (!hasStartedSpeaking) {
 						hasStartedSpeaking = true;
 						stopAllAudio();
 					}
-
 					lastSoundTime = Date.now();
 				}
 
-				// Start silence detection only after initial speech/noise has been detected
-				if (hasStartedSpeaking) {
-					if (Date.now() - lastSoundTime > 2000) {
-						confirmed = true;
-
-						if (mediaRecorder) {
-							console.log('%c%s', 'color: red; font-size: 20px;', 'ðŸ”‡ Silence detected');
-							mediaRecorder.stop();
-							return;
-						}
-					}
-				}
-
 				window.requestAnimationFrame(processFrame);
 			};
 
@@ -358,10 +392,6 @@
 		detectSound();
 	};
 
-	let finishedMessages = {};
-	let currentMessageId = null;
-	let currentUtterance = null;
-
 	const speakSpeechSynthesisHandler = (content) => {
 		if ($showCallOverlay) {
 			return new Promise((resolve) => {
@@ -432,7 +462,7 @@
 		assistantSpeaking = false;
 		interrupted = true;
 
-		if (chatStreaming) {
+		if (chatStreaming && ($settings?.voiceInterruption ?? false)) {
 			stopResponse();
 		}
 
@@ -449,6 +479,134 @@
 		}
 	};
 
+	// Upload queue and finalization for chunked mode
+	const processQueue = () => {
+		while (inFlight < MAX_CONCURRENT_UPLOADS && uploadQueue.length > 0) {
+			const blob = uploadQueue.shift();
+			inFlight++;
+			const send = async (b: Blob, attempt = 1) => {
+				try {
+					let type = b.type || mediaRecorder?.mimeType || 'audio/webm';
+					let ext = 'webm';
+					if (type.startsWith('audio/')) {
+						ext = type.split('/')[1].split(';')[0] || 'webm';
+					}
+					const file = blobToFile(b, `chunk-${Date.now()}.${ext}`);
+					const res = await transcribeAudio(
+						localStorage.token,
+						file,
+						$settings?.audio?.stt?.language
+					).catch((error) => {
+						return Promise.reject(error);
+					});
+					if (res && res.text) {
+						transcriptBuffer.push(res.text);
+					}
+				} catch (err) {
+					if (attempt < 3) {
+						const delay = Math.pow(2, attempt) * 200;
+						setTimeout(() => send(blob, attempt + 1), delay);
+						return;
+					} else {
+						if (!warnedDrop) {
+							toast.error(`${err}`);
+							warnedDrop = true;
+						}
+					}
+				} finally {
+					inFlight--;
+					processQueue();
+				}
+			};
+			send(blob);
+		}
+	};
+
+	const waitForUploadsToFinish = async () => {
+		return new Promise<void>((resolve) => {
+			const check = () => {
+				if (uploadQueue.length === 0 && inFlight === 0) {
+					resolve();
+				} else {
+					setTimeout(check, 100);
+				}
+			};
+			check();
+		});
+	};
+
+	const finalizeCall = async () => {
+		if (stopping) return;
+		stopping = true;
+		try {
+			if (chunkIntervalId) {
+				clearInterval(chunkIntervalId);
+				chunkIntervalId = null;
+			}
+			if (mediaRecorder) {
+				try {
+					if (!isIPhone && mediaRecorder.state === 'recording') {
+						mediaRecorder.requestData();
+					}
+				} catch (e) {
+					console.error(e);
+				}
+				try {
+					mediaRecorder.stop();
+				} catch (e) {
+					console.log(e);
+				}
+			}
+			if (isIPhone) {
+				await new Promise((r) => setTimeout(r, 150));
+				try {
+					const type = mediaRecorder?.mimeType || 'audio/mp4';
+					let ext = 'mp4';
+					if (type.startsWith('audio/')) {
+						ext = type.split('/')[1].split(';')[0] || 'mp4';
+					}
+					const finalBlob = new Blob(audioChunks, { type });
+					const file = blobToFile(finalBlob, `call-${Date.now()}.${ext}`);
+					const res = await transcribeAudio(
+						localStorage.token,
+						file,
+						$settings?.audio?.stt?.language
+					).catch((error) => {
+						toast.error(`${error}`);
+						return null;
+					});
+					if (res && res.text) {
+						const _responses = await submitPrompt(res.text, { _raw: true });
+						console.log(_responses);
+					}
+				} finally {
+					audioChunks = [];
+				}
+			} else {
+				await waitForUploadsToFinish();
+				const finalTranscript = (transcriptBuffer.join(' ') || '').trim();
+				if (finalTranscript.length > 0) {
+					const _responses = await submitPrompt(finalTranscript, { _raw: true });
+					console.log(_responses);
+				}
+			}
+		} finally {
+			if (wakeLock) {
+				try {
+					await wakeLock.release();
+				} catch (e) {
+					console.log(e);
+				}
+				wakeLock = null;
+			}
+			if (audioStream) {
+				const tracks = audioStream.getTracks();
+				tracks.forEach((track) => track.stop());
+				audioStream = null;
+			}
+		}
+	};
+
 	let audioAbortController = new AbortController();
 
 	// Audio cache map where key is the content and value is the Audio object.
@@ -656,6 +814,7 @@
 		return async () => {
 			await stopAllAudio();
 
+			await finalizeCall();
 			stopAudioStream();
 
 			eventTarget.removeEventListener('chat:start', chatStartHandler);
@@ -667,14 +826,14 @@
 
 			await stopAllAudio();
 
-			await stopRecordingCallback(false);
+			// finalizeCall already called above; ensure camera stops
 			await stopCamera();
 		};
 	});
 
 	onDestroy(async () => {
 		await stopAllAudio();
-		await stopRecordingCallback(false);
+		await finalizeCall();
 		await stopCamera();
 
 		await stopAudioStream();
@@ -974,6 +1133,7 @@
 				<button
 					class=" p-3 rounded-full bg-gray-50 dark:bg-gray-900"
 					on:click={async () => {
+						await finalizeCall();
 						await stopAudioStream();
 						await stopVideoStream();
 
